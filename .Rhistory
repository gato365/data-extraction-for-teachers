geo_response <- req_perform(request(geo_url))
# Step 1d: If the status code is 200 (OK), use resp_body_json() to parse our response and as.data.frame to coerce it to data.frame.
if (resp_status(geo_response) == 200) {
geo_data <- resp_body_json(geo_response) |>
as.data.frame()
# Step 1e: Assess if the output has 0 length, meaning no result. If so, stop and display an error message.
if (length(geo_data) == 0) {
stop("City not found. Please check the city name.")
}
# Step 1f: Assign latitude and longitude to variables, and use round() to clip it down to 2 decimal places.
lat <- round(geo_data$lat, digits = 2)
lon <- round(geo_data$lon, digits = 2)
# Optional: Print a string displaying the city name and latitude / longitude.
cat("Coordinates for", city_name, "-> Latitude:", lat, "Longitude:", lon, "\n")
# Step 2: Use the One Call API to get the past 5 days of weather data
# Step 2a: Define the date range using variable 'numdays'
date_range <- as.character(lubridate::today() - days(1:numdays))
# Step 2b: Initialize data frame to hold the outputs. "hist_weather" for historical weather data.
hist_weather <- data.frame()
# Step 2c: Loop over dates and make an API call for each day. For every date in the date vector, supply latitude, longitude, the different date, API key, and provide unit preference.
for (date in date_range) {
weather_url <- glue(
"https://api.openweathermap.org/data/3.0/onecall/day_summary?",
"lat=", lat,
"&lon=", lon,
"&date=", date,
"&appid=", Sys.getenv("API_KEY"),
"&units=imperial"
)
# Step 2d: Make the API call using the different weather_url queries for each date. Store these in weather_response.
weather_response <- req_perform(request(weather_url))
# Step 2e: Use logic to evaluate the response and use fromJSON() to get the content from the JSON output and use "flatten = TRUE" to unnest the data.
if (resp_status(weather_response) == 200) {
daily_weather_parsed <- resp_body_json(weather_response) |>
as.data.frame()
# Step 2f: Add date and city name columns using mutate()
daily_weather_parsed <- daily_weather_parsed |>
mutate(
city =city_name,
date = date)
# Step 2g: Use bind_rows to add all the rows to the hist_weather data frame.
hist_weather <- bind_rows(hist_weather, daily_weather_parsed)
} else {
# Step 2i: Use logic (else) to print an error message for when weather data is not obtained.
warning(paste("Failed to get weather for", date, "-", resp_status(weather_response)))
}
}
print(hist_weather)
} else {
stop("Geocoding failed. Check your API key or city name.")
}
if (resp_status(response) == 200) {
# Parse and flatten JSON
result <- resp_body_json(response)
# Convert to data frame directly
currweather <- as.data.frame(result)
print(select(currweather,name, coord.lon, coord.lat, weather.main, main.temp))
} else {
cat("Failed. Status code:", resp_status(response), "\n")
}
geocode("Ames, IA, USA")
prev_weather <- function(city, numdays){
# Step 1: Use Geocoding API to get lattitude and longitude
# Step 1a: Construct URL query using city name and API key
geo_url <- glue(
"http://api.openweathermap.org/geo/1.0/direct?",
"q=", URLencode(city_name),
"&limit=1&appid=", Sys.getenv("API_KEY")
)
# Step 1c:
geo_response <- req_perform(request(geo_url))
# Step 1d: If the status code is 200 (OK), use resp_body_json() to parse our response and as.data.frame to coerce it to data.frame.
if (resp_status(geo_response) == 200) {
geo_data <- resp_body_json(geo_response) |>
as.data.frame()
# Step 1e: Assess if the output has 0 length, meaning no result. If so, stop and display an error message.
if (length(geo_data) == 0) {
stop("City not found. Please check the city name.")
}
# Step 1f: Assign latitude and longitude to variables, and use round() to clip it down to 2 decimal places.
lat <- round(geo_data$lat, digits = 2)
lon <- round(geo_data$lon, digits = 2)
# Optional: Print a string displaying the city name and latitude / longitude.
cat("Coordinates for", city_name, "-> Latitude:", lat, "Longitude:", lon, "\n")
# Step 2: Use the One Call API to get the past 5 days of weather data
# Step 2a: Define the date range using variable 'numdays'
date_range <- as.character(lubridate::today() - days(1:numdays))
# Step 2b: Initialize data frame to hold the outputs. "hist_weather" for historical weather data.
hist_weather <- data.frame()
# Step 2c: Loop over dates and make an API call for each day. For every date in the date vector, supply latitude, longitude, the different date, API key, and provide unit preference.
for (date in date_range) {
weather_url <- glue(
"https://api.openweathermap.org/data/3.0/onecall/day_summary?",
"lat=", lat,
"&lon=", lon,
"&date=", date,
"&appid=", Sys.getenv("API_KEY"),
"&units=imperial"
)
# Step 2d: Make the API call using the different weather_url queries for each date. Store these in weather_response.
weather_response <- req_perform(request(weather_url))
# Step 2e: Use logic to evaluate the response and use fromJSON() to get the content from the JSON output and use "flatten = TRUE" to unnest the data.
if (resp_status(weather_response) == 200) {
daily_weather_parsed <- resp_body_json(weather_response) |>
as.data.frame()
# Step 2f: Add date and city name columns using mutate()
daily_weather_parsed <- daily_weather_parsed |>
mutate(
city =city_name,
date = date)
# Step 2g: Use bind_rows to add all the rows to the hist_weather data frame.
hist_weather <- bind_rows(hist_weather, daily_weather_parsed)
} else {
# Step 2i: Use logic (else) to print an error message for when weather data is not obtained.
warning(paste("Failed to get weather for", date, "-", resp_status(weather_response)))
}
}
return(hist_weather)
} else {
stop("Geocoding failed. Check your API key or city name.")
}
}
prev_weather("San Luis Obispo", 4)
#sf_map <- function(cities)
cities <- c("San Luis Obispo", "Santa Barbara", "San Francisco")
# Function to get coordinates + weather summary
get_city_weather <- function(city, date = Sys.Date()) {
geo_url <- glue(
"http://api.openweathermap.org/geo/1.0/direct?",
"q=", URLencode(city),
"&limit=1&appid=", Sys.getenv("API_KEY")
)
geo_response <- req_perform(request((geo_url)))
if (resp_stauts(geo_response) == 200) {
geo_data <- resp_body_json(geo_response)
if (length(geo_data) == 0) return(NULL)
lat <- geo_data$lat
lon <- geo_data$lon
weather_url <- glue(
"https://api.openweathermap.org/data/3.0/onecall/day_summary?",
"lat=", lat,
"&lon=", lon,
"&date=", format(date, "%Y-%m-%d"),
"&appid=", Sys.getenv("API_KEY"),
"&units=imperial"
)
weather_response <- req_perform(request((weather_url)))
if (resp_stats(weather_response) == 200) {
weather_data <- (resp_body_json(weather_response))
tibble(
city = city,
date = date,
lat = lat,
lon = lon,
temp_max = weather_data$temperature$max,
temp_min = weather_data$temperature$min
)
} else return(NULL)
} else return(NULL)
}
# Fetch for all cities
weather_df <- bind_rows(lapply(cities, get_city_weather))
#sf_map <- function(cities)
cities <- c("San Luis Obispo", "Santa Barbara", "San Francisco")
# Function to get coordinates + weather summary
get_city_weather <- function(city, date = Sys.Date()) {
geo_url <- glue(
"http://api.openweathermap.org/geo/1.0/direct?",
"q=", URLencode(city),
"&limit=1&appid=", Sys.getenv("API_KEY")
)
geo_response <- req_perform(request((as.string(geo_url))))
if (resp_status(geo_response) == 200) {
geo_data <- resp_body_json(geo_response)
if (length(geo_data) == 0) return(NULL)
lat <- geo_data$lat
lon <- geo_data$lon
weather_url <- glue(
"https://api.openweathermap.org/data/3.0/onecall/day_summary?",
"lat=", lat,
"&lon=", lon,
"&date=", format(date, "%Y-%m-%d"),
"&appid=", Sys.getenv("API_KEY"),
"&units=imperial"
)
weather_response <- req_perform(request((weather_url)))
if (resp_status(weather_response) == 200) {
weather_data <- (resp_body_json(weather_response))
tibble(
city = city,
date = date,
lat = lat,
lon = lon,
temp_max = weather_data$temperature$max,
temp_min = weather_data$temperature$min
)
} else return(NULL)
} else return(NULL)
}
# Fetch for all cities
weather_df <- bind_rows(lapply(cities, get_city_weather))
#sf_map <- function(cities)
cities <- c("San Luis Obispo", "Santa Barbara", "San Francisco")
# Function to get coordinates + weather summary
get_city_weather <- function(city, date = Sys.Date()) {
geo_url <- glue(
"http://api.openweathermap.org/geo/1.0/direct?",
"q=", URLencode(city),
"&limit=1&appid=", Sys.getenv("API_KEY")
)
geo_response <- req_perform(request((as.character(geo_url))))
if (resp_status(geo_response) == 200) {
geo_data <- resp_body_json(geo_response)
if (length(geo_data) == 0) return(NULL)
lat <- geo_data$lat
lon <- geo_data$lon
weather_url <- glue(
"https://api.openweathermap.org/data/3.0/onecall/day_summary?",
"lat=", lat,
"&lon=", lon,
"&date=", format(date, "%Y-%m-%d"),
"&appid=", Sys.getenv("API_KEY"),
"&units=imperial"
)
weather_response <- req_perform(request((weather_url)))
if (resp_status(weather_response) == 200) {
weather_data <- (resp_body_json(weather_response))
tibble(
city = city,
date = date,
lat = lat,
lon = lon,
temp_max = weather_data$temperature$max,
temp_min = weather_data$temperature$min
)
} else return(NULL)
} else return(NULL)
}
# Fetch for all cities
weather_df <- bind_rows(lapply(cities, get_city_weather))
#sf_map <- function(cities)
cities <- c("San Luis Obispo", "Santa Barbara", "San Francisco")
# Function to get coordinates + weather summary
get_city_weather <- function(city, date = Sys.Date()) {
geo_url <- glue(
"http://api.openweathermap.org/geo/1.0/direct?",
"q=", URLencode(city),
"&limit=1&appid=", Sys.getenv("API_KEY")
)
geo_response <- req_perform(request((geo_url)))
if (resp_status(geo_response) == 200) {
geo_data <- resp_body_json(geo_response)
if (length(geo_data) == 0) return(NULL)
lat <- geo_data$lat
lon <- geo_data$lon
weather_url <- glue(
"https://api.openweathermap.org/data/3.0/onecall/day_summary?",
"lat=", lat,
"&lon=", lon,
"&date=", format(date, "%Y-%m-%d"),
"&appid=", Sys.getenv("API_KEY"),
"&units=imperial"
)
weather_response <- req_perform(request((weather_url)))
if (resp_status(weather_response) == 200) {
weather_data <- (resp_body_json(weather_response))
tibble(
city = city,
date = date,
lat = lat,
lon = lon,
temp_max = weather_data$temperature$max,
temp_min = weather_data$temperature$min
)
} else return(NULL)
} else return(NULL)
}
# Fetch for all cities
weather_df <- bind_rows(lapply(cities, get_city_weather))
prev_weather("San Luis Obispo", 4)
# Step 1c:
# Use req_perform() and request() to call the API with the URL request
geo_response <- req_perform(request(geo_url))
# Step 1d: If the status code is 200 (OK), use resp_body_json() to parse our response and as.data.frame to coerce it to data.frame.
if (resp_status(geo_response) == 200) {
geo_data <- resp_body_json(geo_response) |>
as.data.frame()
# Step 1e: Assess if the output has 0 length, meaning no result. If so, stop and display an error message.
if (length(geo_data) == 0) {
stop("City not found. Please check the city name.")
}
# Step 1f: Assign latitude and longitude to variables, and use round() to clip it down to 2 decimal places.
lat <- round(geo_data$lat, digits = 2)
lon <- round(geo_data$lon, digits = 2)
# Optional: Print a string displaying the city name and latitude / longitude.
cat("Coordinates for", city_name, "-> Latitude:", lat, "Longitude:", lon, "\n")
# Step 2: Use the One Call API to get the past 5 days of weather data
# Step 2a: Define the date range using variable 'numdays'
date_range <- as.character(lubridate::today() - days(1:numdays))
# Step 2b: Initialize data frame to hold the outputs. "hist_weather" for historical weather data.
hist_weather <- data.frame()
# Step 2c: Loop over dates and make an API call for each day. For every date in the date vector, supply latitude, longitude, the different date, API key, and provide unit preference.
for (date in date_range) {
weather_url <- glue(
"https://api.openweathermap.org/data/3.0/onecall/day_summary?",
"lat=", lat,
"&lon=", lon,
"&date=", date,
"&appid=", Sys.getenv("API_KEY"),
"&units=imperial"
)
# Step 2d: Make the API call using the different weather_url queries for each date. Store these in weather_response.
weather_response <- req_perform(request(weather_url))
# Step 2e: Use logic to evaluate the response and use fromJSON() to get the content from the JSON output and use "flatten = TRUE" to unnest the data.
if (resp_status(weather_response) == 200) {
daily_weather_parsed <- resp_body_json(weather_response) |>
as.data.frame()
# Step 2f: Add date and city name columns using mutate()
daily_weather_parsed <- daily_weather_parsed |>
mutate(
city =city_name,
date = date)
# Step 2g: Use bind_rows to add all the rows to the hist_weather data frame.
hist_weather <- bind_rows(hist_weather, daily_weather_parsed)
} else {
# Step 2i: Use logic (else) to print an error message for when weather data is not obtained.
warning(paste("Failed to get weather for", date, "-", resp_status(weather_response)))
}
}
print(hist_weather)
} else {
stop("Geocoding failed. Check your API key or city name.")
}
prev_weather("San Luis Obispo", 4)
knitr::opts_chunk$set(echo = TRUE)
#| message: false
#| warning: false
library(rvest)
library(httr)
library(dplyr)
library(stringr)
library(rlang)
library(purrr)
library(ggplot2)
## Creation of URL
team_name <- "was"
year <- 2023
generic_url <- paste0("https://www.pro-football-reference.com/teams/",team_name,"/",year,".htm#all_games")
## Special Note 1: Maybe Optional (Further Research is needed on its relevance)
# request_details <- "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
# response <- GET(url, user_agent(request_details),timeout(20))
webpage <- generic_url %>%
rvest::read_html()
web_tables <- webpage %>%
rvest::html_table()
int_web_table <- web_tables %>%
purrr::pluck(2)
inital_col_names <- colnames(int_web_table)
first_row_names <- slice(int_web_table, 1)
View(int_web_table)
inital_col_names <- colnames(int_web_table)
first_row_names <- slice_head(int_web_table)
inital_col_names <- colnames(int_web_table)
first_row_names <- int_web_table[1,]
# CANNOT USE SLICE: CANNOT TRANSFORM A DATA FRAME WITH NA OR "" NAMES
new_colnames <- paste(inital_col_names,first_row_names,sep="_") %>%
str_trim() %>%
str_to_lower()
new_colnames[6] <- "game_location"
new_colnames[21] <- "passing_yds_sk"
colnames(int_web_table) <- new_colnames
web_table1 <- int_web_table %>%
dplyr::slice(2:18)
knitr::opts_chunk$set(echo = TRUE)
#| message: false
#| warning: false
library(rvest)
library(httr)
library(dplyr)
library(stringr)
library(rlang)
library(purrr)
library(ggplot2)
## Creation of URL
team_name <- "was"
year <- 2023
generic_url <- paste0("https://www.pro-football-reference.com/teams/",team_name,"/",year,".htm#all_games")
## Special Note 1: Maybe Optional (Further Research is needed on its relevance)
# request_details <- "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
# response <- GET(url, user_agent(request_details),timeout(20))
webpage <- generic_url %>%
rvest::read_html()
web_tables <- webpage %>%
rvest::html_table()
int_web_table <- web_tables %>%
purrr::pluck(2)
inital_col_names <- colnames(int_web_table)
first_row_names <- int_web_table[1,]
# CANNOT USE SLICE: CANNOT TRANSFORM A DATA FRAME WITH NA OR "" NAMES
new_colnames <- paste(inital_col_names,first_row_names,sep="_") %>%
str_trim() %>%
str_to_lower()
new_colnames[6] <- "game_location"
new_colnames[21] <- "passing_yds_sk"
colnames(int_web_table) <- new_colnames
web_table1 <- int_web_table %>%
dplyr::slice(2:18)
inital_col_names <- colnames(int_web_table)
first_row_names <- int_web_table[1,]
# CANNOT USE SLICE: CANNOT TRANSFORM A DATA FRAME WITH NA OR "" NAMES
new_colnames <- paste(inital_col_names,first_row_names,sep="_") %>%
str_trim() %>%
str_to_lower()
new_colnames[6] <- "game_location"
new_colnames[21] <- "passing_yds_sk"
inital_col_names <- colnames(int_web_table)
first_row_names <- int_web_table[1,]
# CANNOT USE SLICE: CANNOT TRANSFORM A DATA FRAME WITH NA OR "" NAMES
new_colnames <- glue(inital_col_names,first_row_names,sep="_") %>%
str_trim() %>%
str_to_lower()
inital_col_names <- colnames(int_web_table)
first_row_names <- int_web_table[1,]
# CANNOT USE SLICE: CANNOT TRANSFORM A DATA FRAME WITH NA OR "" NAMES
new_colnames <- paste(inital_col_names,first_row_names,sep="_") %>%
str_trim() %>%
str_to_lower()
colnames(int_web_table) <- new_colnames
new_colnames[6] <- "game_location"
new_colnames[21] <- "passing_yds_sk"
colnames(int_web_table) <- new_colnames
web_table1 <- int_web_table %>%
dplyr::slice(2:18)
View(int_web_table)
knitr::opts_chunk$set(echo = TRUE)
#| message: false
#| warning: false
library(rvest)
library(httr)
library(dplyr)
library(stringr)
library(rlang)
library(purrr)
library(ggplot2)
## Creation of URL
team_name <- "was"
year <- 2023
generic_url <- paste0("https://www.pro-football-reference.com/teams/",team_name,"/",year,".htm#all_games")
## Special Note 1: Maybe Optional (Further Research is needed on its relevance)
# request_details <- "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
# response <- GET(url, user_agent(request_details),timeout(20))
webpage <- generic_url %>%
rvest::read_html()
web_tables <- webpage %>%
rvest::html_table()
int_web_table <- web_tables %>%
purrr::pluck(2)
inital_col_names <- colnames(int_web_table)
first_row_names <- int_web_table[1,]
# CANNOT USE SLICE: CANNOT TRANSFORM A DATA FRAME WITH NA OR "" NAMES
new_colnames <- paste(inital_col_names,first_row_names,sep="_") %>%
str_trim() %>%
str_to_lower()
new_colnames[6] <- "game_location"
new_colnames[21] <- "passing_yds_sk"
colnames(int_web_table) <- new_colnames
web_table1 <- int_web_table %>%
dplyr::slice(2:18)
View(int_web_table)
knitr::opts_chunk$set(echo = TRUE)
#| message: false
#| warning: false
library(rvest) ## Web scraping
library(dplyr) ## Data manipulation
library(stringr) ## String modification
library(rlang)
library(purrr) ## Enchanced iteration
library(ggplot2) ## Visualizations
## Creation of URL
team_name <- "was"
year <- 2023
generic_url <- paste0("https://www.pro-football-reference.com/teams/",team_name,"/",year,".htm#all_games")
## Special Note 1: Maybe Optional (Further Research is needed on its relevance)
# request_details <- "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
# response <- GET(url, user_agent(request_details),timeout(20))
webpage <- generic_url %>%
rvest::read_html()
web_tables <- webpage %>%
rvest::html_table()
int_web_table <- web_tables %>%
purrr::pluck(2)
inital_col_names <- colnames(int_web_table)
first_row_names <- int_web_table[1,]
# CANNOT USE SLICE: CANNOT TRANSFORM A DATA FRAME WITH NA OR "" NAMES
new_colnames <- paste(inital_col_names,first_row_names,sep="_") %>%
str_trim() %>%
str_to_lower()
new_colnames[6] <- "game_location"
new_colnames[21] <- "passing_yds_sk"
colnames(int_web_table) <- new_colnames
web_table1 <- int_web_table %>%
dplyr::slice(2:18)
View(web_table3)
View(web_tables)
#| output: false
library(httr2) ## Updated version of httr, makes https requests
library(tibble) ## Tidyverse version of data.frame
library(lubridate) ## Time and date handling
library(ggplot2) ## Visualizations
library(dplyr) ## Data manipulation
library(dotenv) ## Load environment variables
library(glue) ## Attach strings together
