---
title: "Session 1: Introduction To Extraction Workshop - HTML & APIs"
author: "Immanuel Williams & Ciera Millard"
date: "`r Sys.Date()`"
format: html
---

## Session 1: Introduction to Data Extraction

### 0. Logisitcs

- CourseKata
- organization of workshop of lecture and questions throughout


### 1. Goals & Objectives (Presentation)


1. Understand the importance of extracting **dynamic data** (via HTML and APIs) in modern data analysis and teaching
2. Learn how to access and work with **APIs** to retrieve structured, real-time data
3. Learn how to bring HTML-based data (e.g., web tables) into R using scraping tools
4. Apply these skills through **hands-on coding practice** and discussion of classroom integration




#### Goals for Introduction

(need to place)

### 2. Conceptual Foundation


- P1. My mentor, Allan, says ask good questions...

- P2. Statistical Question: **Who had the most impactful first season in terms of points: Michael Jordan, LeBron James, or Kobe Bryant?**

- P3. I recently submitted a manuscript on this exact dataset, so let’s use it as our starting point.

- P4. We’ll begin by working with a static Excel file that contains per-game stats for each player’s 15 seasons in the NBA. 


- P5. Let's Load the pertinent libraries


- T1. ____ Fill in the code (chatgpt)

```{r message = FALSE, warning = FALSE}
library(readxl)      ## Data Extraction      --- E
library(dplyr)       ## Data Transformation  --- T
library(ggplot2)     ## Data Visualization   --- V
```

- P6. Load in the data

- T2. ____ Fill in the code (chatgpt)
```{r}
nba_df <- read_xlsx("nba_data.xlsx", sheet = "modern_nba_legends_08302019")
```


- P7. Let's view the data ...

- T3. Use the `glimpse` function to view the data
```{r}
glimpse(nba_df)
```


- Q1. Look through the data, does it look clean? Discuss amongst  Your peers. (chatgpt)

- Ans Q1: It is clean the numeric variables are supposed to be numeric and the characrets variables are treated as char (chatgpt)



- P8. Now let's clean the focus on the data frame that we are after

- T4. ____ Fill in the code (chatgpt)
```{r}
season_1_df <- nba_df %>% 
  filter(Season == "season_1")
```


- P9. Now lets look at a plot of their points (chatgpt)

T5. ____ Fill in the code (chatgpt)
```{r}
season_1_df %>% 
  ggplot(aes(x = Name, y = PTS)) +
  geom_boxplot() +
  theme_bw() 
```
- Note 1: We could have spruced it up but here we just wanted to answer the question, if you have the urge please do so.

- Q2. What conclusion could be made (chatgpt)


P10. Now what about, Magic Johnson or Wilt Chamberlain
Maybe Luka Dončić or Ja Morant 
If I wanted to add this data I need to go to the originnal source not an excel sheet to do this (chatgpt)


Note 2:
-   Shift students from being passive data users to **active data seekers**
-   Move beyond the idea of “waiting for clean data” to *learning how to access, validate, and clean it* themselves
-   Teach both the **skill to extract** and the **capacity to teach extraction**


Note 3:
-   **Why this matters:** We as instructors should not just rely on pre-built packages or static datasets. The digital world changes constantly — websites, APIs, and file structures evolve.

-   **Our responsibility:** Teach students (and ourselves) how to adapt and access real-world data sources. Equip learners with *skills to extract*, not just *consume pre-extracted content*.

-   Despite the growing importance of live data, most introductory courses still rely heavily on static, pre-cleaned datasets. This limits students' exposure to the realities of modern data work.


-   **Key idea:** *Flat files can still be dynamic* depending on how they’re maintained — but we use the term "dynamic" here to emphasize **external, real-time data access** through APIs and web scraping.

-   The availability of dynamic, frequently updated data — especially via web APIs — has grown exponentially in recent years. This shift demands new strategies in how we teach data access.


### A. Static Files or Sources Extraction 


- P11. 

-   Examples: CSV, Excel files

-   Typically unchanging unless manually edited

-   Often pre-loaded into classroom activities

-   May still require cleaning (e.g., column names, missing data)

- Note 4: Messy data is not always a bad thing

### B. Dynamic Sources Extraction

- P12.

-    **Definition**: Data sources that update over time or are externally controlled (i.e., you don’t own the source)


- P13. 

-   **Two primary types:**

    1.  Application Programming Interface **APIs** – Designed to serve structured data upon request (e.g., player stats, weather)
    2. Hypertext Markup Language **HTML/Web Pages** – Seen as dynamic when content changes (especially sports, news, etc.)

-   HTML pages are primarily designed for human readability, while APIs are designed for structured machine access. Both offer pathways to dynamic data, each with different advantages and challenges.


-   Bridging the gap between classroom exercises and real-world data practice requires that students learn not just how to analyze data — but how to find it, extract it, and prepare it themselves.



- Note 5: HTML *can be treated* as static or dynamic depending on how frequently the page updates. For this workshop, **we treat HTML as dynamic**, especially for sports data.









#### 2.b. What are Web APIs?

Note 6: 

- There are many kinds of APIs, but in this workshop, we’ll focus specifically on **web APIs** — tools designed to let us request and retrieve data from online sources.
- In R, we’ll act like a piece of software making those requests, allowing us to access live data programmatically.





![Flow of Data via API (vrogue.co)](images/clipboard-1951848651.jpeg)


- P14.

- API stand for  Application Programming Interfaces

- It is a way for software to communicate with one another

- One way it work is that it allow programs to request data directly from external servers in a structured format (most often JSON). 

```
{
  "player": "LeBron James",
  "points": 27.1,
  "team": "Lakers"
}
```
- The **keys** are `players`, `points `and `team`
- The **values** for the corresponding keys are `LeBron James`, `27.1`, `Lakers`


Note 7: 

- There are a lot of acronyms 
- JSON - Java Script Object Notation - javascript is web developing software (chatgpt)



Q3. What does JSON?
Answer: Java script object notation 


Note 8:

- Describe Image `Flow of Data via API (vrogue.co)`
- A user sends a request via the internet → the API talks to the server → the server queries the database → the API responds with data, often as JSON.


- P15.

- Learning to work with web APIs teaches students more than just how to extract data — it gives them the tools to:

  - Locate relevant APIs (e.g., weather, sports, music)
  - Construct and test their own API requests
  - Interpret JSON responses (including nested structures)
  - Transform the results into tidy formats ready for analysis

**P16**

- APIs aren't just technical tools, they’re increasingly the primary way to access and query data stored in external databases.

- In today’s fast-changing digital environment, students must be equipped to retrieve and work with information from live, external sources, not just rely on pre-cleaned datasets.

- Note 9:

This is what pushes students from passive observers of data to active agents in its collection, structure, and use. It aligns closely with what real-world data science jobs require, especially when you're no longer just analyzing data, but acquiring it.





[![Volume of Data Created (Statista)](images/clipboard-272118191.png)](https://www.statista.com/statistics/871513/worldwide-data-created/)

The use of APIs requires keys, which are unique and secret codes that are used to authorize your request and identify your user and billing information. Consequently, keeping these codes secret is imperative. To do so, store API keys in environment files which reside on your computer, and not coded into variables or available in plain text on your working files.




show images creating an app (provide my youtube video)
  - get id and secret (blurr it out)
(make highlighe secret and ID on image )


[![Volume of Data Created (Statista)](images/spotify_id.png)

id: (eman post in some way)
secret: (eman post on screen)

Note 10:
Note all APIs website work the same, getting experience with 1-2, you start to develop a philosophy on how to approach them (chatgpt)




Code 2



**Step 1: Load the Required Packages**

```{r}
library(spotifyr)
library(dplyr)
```

---

**Step 2: Set Up Credentials**

Before using the Spotify API, you need to create an app on the  and retrieve your **Client ID** and **Client Secret**. (I have a video here to do this on your own but on the screen, you can see the id and secret - (chatgpt))[How to make app]("")

```{r}
# Replace these with your actual credentials (do NOT share them publicly)
client_id <- 'your_spotify_client_id_here'
client_secret <- 'your_spotify_client_secret_here'
```

---

**Step 3: Get an Access Token**

The token allows Spotify to recognize both **who you are** and **what you're requesting**.

```{r}
access_token <- get_spotify_access_token(
  client_id = client_id,
  client_secret = client_secret
)
```

---

**Step 4: Request Artist Data**

Now that you're authenticated, you can request track-level data from Spotify's API.

```{r}
beatles_df <- get_artist_audio_features('the beatles')
```

---

**Step 5: Explore the Dataset**

Use `glimpse()` to quickly examine the structure of the data.

```{r}
glimpse(beatles_df)
```

---

**Step 6: Try Another Artist**

Try retrieving audio features for a different artist.

```{r}
taylor_df <- get_artist_audio_features('taylor swift')
glimpse(taylor_df)
```



Note 11:
How databases and access can change in that the developers may change access or change the way you access information so you have to be aware of this. (chatgpt)

Data types are usually sotred correctly as chars or numerics so noy much cleaning required (chatgpt)



------------------------------------------------------------------------




# EMAN IMPORTANT - Provide 2-3 webscrape
Start here
#### 2.c. HTML Web Scraping

![Process of HTML Scraping (sstm2.com)](images/clipboard-258030846.jpeg){width="722"}

Much like APIs, lots of relevant and useful information is available directly on webpages, which are readable by humans rather than APIs which are designed for machine access. By learning this skill, students are able to:

-   Locate relevant sources (e.g., sports data from Pro Football Reference)

-   Understand how websites deliver and organize content

-   Transform and clean data for analysis and visualization

Often times, HTML tables contain unexpected structures or data types (images, links, etc) and can present a challenge that develops not only data cleaning skills, but intention, planning, and adaptability when handling and analyzing difficult data.








------------------------------------------------------------------------

#### 2.d. Our Focus: Extraction

While analyzing data is comprised of many steps and processes, one of the first and most foundational of these is *extracting the data*. Here are some common ways data can be extracted. In this workshop, the focus will be on the last two methods.

-   CSV (comma-seperated values) files

    -   `read.csv`

    -   `readr::read_csv`

-   Excel files (.xlsx, .xlsb, .xlsm, .xls)

    -   `readxl::read_excel`

        -   Not .xlsb

    -   `readxlsb::read_xlsb`

    -   `tidyxl::xlsx_cells`

-   Pre-loaded data frames in R (mtcars)

    -   `data()`

-   URLs to GitHub repositories

    -   `readr::read_csv("link")`

-   **HTML (Web Extraction/Scraping)**

    -   `rvest::read_html`

-   **API requests (Databases)**

    -   `httr2`

#### 2.e. Workshop Agenda

**Session 1: Introduction**

This session serves as a discussion of the principles and reasoning behind learning these concepts and how they can benefit the classroom.

**Session 2: Getting Weather Data via OpenWeather API**

In this session, we dive into OpenWeather API and learn to use packages like httr2 to execute API calls. We will also discuss URLs, queries, data structures, and more.

**Session 3: Scraping NFL Sports Data**

In this session, we will use Pro-Football Reference to learn how to extract and clean HTML table data for use in statistical analysis and visualizations.

**Session 4: Putting it All Together (Project)**

In this project, we will use HTML scraping joined with the OpenWeather API to create our own cloropleth map of Iowa.

![Chloropleth Map (geoapify.com)](images/clipboard-3021085322.png)

------------------------------------------------------------------------

#### (need to delete, place somewhere) Introduction to Libraries & Glossary

This project, when possible, utilizes `tidyverse` packages. `tidyverse` is a collection of open-source packages that are well-integrated to tackle problems of data extraction, manipulation, transformation, exploration, and visualization.

![Collection of Tidyverse packages.](images/clipboard-59929015.png){width="631"}

```{r}
#| output: false
#| eval: false
library(rvest)      # Web scraping
library(dplyr)      # Data manipulation
library(stringr)    # String cleaning
library(rlang)      # Advanced evaluation
library(purrr)      # Functional tools
library(ggplot2)    # Visualizations
library(httr2)      # Makes web requests
library(tibble)     # Easier and prettier data frames
library(lubridate)  # Handles dates
library(dotenv)     # Loads environment variables from .Renviron
library(glue)       # Easier string concatenation
library(tigris)     # U.S. shapefiles for mapping
```

This is a list of R packages used in this project, accompanied by brief descriptions of what they do.

`rvest`

-   Used for web scraping: parses HTML/XML documents into a navigable format.
-   Extracts structured data using CSS selectors, XPath, or tag-based search (`html_table()`, `html_text()`, etc.).

------------------------------------------------------------------------

`dplyr`

-   Core package for tidyverse-style data manipulation (`filter()`, `mutate()`, `select()`, etc.).
-   Supports chaining operations with `%>%` / \|`>`, making data workflows readable and efficient.

------------------------------------------------------------------------

`stringr`

-   Provides a consistent set of functions for string manipulation.
-   Handles pattern matching, replacement, splitting, and formatting.

------------------------------------------------------------------------

`rlang`

-   Supports advanced evaluation and programming with tidyverse tools.
-   Useful when writing custom functions that dynamically reference or modify variables.

------------------------------------------------------------------------

`purrr`

-   Enables functional programming with mapping tools like `map()`, `map_df()`, `walk()`, etc.
-   Replaces (many) for-loops and supports clean iteration over lists and vectors.

------------------------------------------------------------------------

`ggplot2`

-   Graphics package for building layered, flexible visualizations.
-   Supports various plot types, themes, scales, and faceting for data storytelling.

------------------------------------------------------------------------

`httr2`

-   Modern HTTP client designed for tidyverse-like API interaction.
-   Replaces `httr` with a more intuitive and pipeable interface (`request() |> req_perform()`).

------------------------------------------------------------------------

`tibble`

-   A modern rethinking of the `data.frame` that prints cleaner and behaves more predictably.
-   Default in tidyverse workflows; avoids surprises like string-to-factor conversion.

------------------------------------------------------------------------

`lubridate`

-   Simplifies working with dates and times: parsing, formatting, and arithmetic.
-   Makes it easy to extract components (e.g., month, weekday) and perform date math.

------------------------------------------------------------------------

`dotenv`

-   Loads environment variables from a `.env` or `.Renviron` file into R.
-   Keeps sensitive data like API keys out of your scripts and version control.

------------------------------------------------------------------------

`glue`

-   Provides string interpolation (e.g., `glue("Hello {name}")`).
-   Cleaner and safer than `paste()` for building URLs, messages, or SQL queries.

------------------------------------------------------------------------

`tigris`

-   Downloads shapefiles and geographic boundary data (e.g., counties, states) from the U.S. Census Bureau.
-   Returns spatial `sf` data frames, making it easy to map and visualize geographic data.

### 3. Hands-On Coding Activity

-   Extracting from accessible sources such as:

    -   A static `.csv` hosted online (warm-up)
    -   A Wikipedia table using `rvest` and `janitor`

-   Introduce `read_csv()` and `rvest::html_table()`

-   Add cleaning steps to enforce tidy principles (snake_case, correct types)

### 4. Reflection

-   How can you introduce real-world messiness without overwhelming students?
-   How would you scaffold tidy principles at the intro-level?

### 5. Misc. Questions/Ideas

1.  Set expectations and workshop goals
2.  Why data extraction matters: relevance to real-world education
3.  Overview of the layout / table of contents
4.  Discuss libraries used (tidyverse, rvest, httr, etc.)
5.  Best practices (e.g., avoiding hardcoding, consistent comments)
6.  Adapting to changing APIs/websites
7.  Anecdote: Spotify example of lost API access
8.  Explain tidy data: snake_case column names, correct data types
9.  Emphasize code flexibility — developers can change APIs overnight
10. Activity: Scaffolding + Code review using example(s)
