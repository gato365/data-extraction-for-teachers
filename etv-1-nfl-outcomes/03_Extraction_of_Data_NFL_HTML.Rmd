---
title: "Extraction of NFL Data - HTML"
author: "Ciera Millard"
date: "`r Sys.Date()`"
output: html_document
editor_options: 
  markdown: 
    wrap: 72
---

## Session 3: HTML Web Scraping - NFL Data Extraction

Now that you’re familiar with HTML elements and scraping, this activity
will walk through extracting, cleaning, and visualizing NFL team
performance data.

------------------------------------------------------------------------

## Step 1: Load Libraries

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
#| message: false
#| warning: false
library(rvest)      # Web scraping
library(dplyr)      # Data manipulation
library(stringr)    # String cleaning
library(rlang)      # Advanced evaluation
library(purrr)      # Functional tools
library(ggplot2)    # Visualizations
```

*Discussion: Why are some of these packages (like `purrr` or `rlang`)
useful for scraping tasks?*

------------------------------------------------------------------------

## Step 2: Construct and Read URL

We start by creating the target URL for a given team and year.

```{r}
# Step 2: Define team and year
team_name <- "was"
year <- 2023

# Step 2a: Construct full URL
generic_url <- paste0("https://www.pro-football-reference.com/teams/", team_name, "/", year, ".htm#all_games")
```

*Discussion: How could we make this part of a function so it is
reusable?*

*See Also:*
<https://www.pro-football-reference.com/teams/was/2023.htm#games>

------------------------------------------------------------------------

## Step 3: Read and Extract HTML Tables

```{r}
# Step 3: Read HTML page
webpage <- generic_url |> rvest::read_html()
```

-   When you run `read_html(url)`, it returns an HTML node pointer, not
    human-readable content.

<!-- -->

-    This pointer references the structure of the web page in memory,
    but doesn't display actual text or data in R.

<!-- -->

-    If you try to print this object directly, you'll see something such
    as:\
    `webpage[[1]] <pointer: 0x00000225...>`

-   R is showing **memory addresses** of HTML elements, not the content.

<!-- -->

-    This is because the HTML content must still be **parsed** or
    **extracted**.

Use `rvest`!

-   `html_table()` : extracts data from `<table>` elements.

<!-- -->

-    `html_text()` : extracts plain text from HTML nodes.

<!-- -->

-    `html_nodes()` or `html_elements()` : selects multiple nodes using
    CSS or XPath.

<!-- -->

-    `html_element()` : selects a single node.

*Discussion*: *Why do you think web scraping tools separate “structure”
from “content”? What are the pros and cons of working with HTML nodes
directly?*

From the webpage, grab the HTML tables using `rvest::html_table()`.

```{r}
# Step 3a: Extract all HTML tables
web_tables <- webpage |> rvest::html_table()
```

The result is a list containing HTML table elements.

*Discussion: What does this data structure look like?*

Select the desired table, the 2023 Regular Season Table, which is the
second table on the webpage. Use `purrr::pluck()` to select the table.

```{r}
# Step 3b: Pick the regular season game table (check structure visually)
int_web_table <- web_tables |> purrr::pluck(2)
```

*Discussion: Why might this index (2) break in the future? What
alternatives could we use to select the correct table more reliably?*

------------------------------------------------------------------------

## Step 4: Clean and Prepare Column Names

Our first row contains more information regarding the columns than the
header of the actual table. The merged cells in the header end up being
repeated over the entire column group they represent, without providing
useful information.

```{r}
# Step 4a: Use first row as column names + clean them
firstrow_names <- int_web_table[1, ] |> unlist() |> as.character()
```

```{r}
# Step 4b: Assign as column names
colnames(int_web_table) <- firstrow_names
```

```{r}
# Step 4c: Remove the first row (it's now the header)
table_1 <- int_web_table[-1, ]
```

```{r}
# Step 4d: Clean the column names with janitor
table_2 <- janitor::clean_names(table_1)
```

```{r}
# Step 4e: Fix problem cases with no useful data within the header or first rows
table_3 <- table_2 |> 
  rename(
    result = x_3,
    game_location = x_4
)
```

*Discussion: What are the risks or tradeoffs in hardcoding columns like
`result` and `game_location`? How could this break?*

------------------------------------------------------------------------

## Step 5: Clean Table and Convert Data Types

```{r}
# Step 5: Drop irrelevant columns and rows, keep only valid games
table_4 <- table_3 |> 
  select(!(x:x_2)) |> 
  filter(opp != "Bye Week")

```

```{r}
# Step 5a: Convert numeric-looking strings to numeric
table_5 <- table_4 |>  
  mutate(across(where(~ all(grepl("^\\s*-?\\d*\\.?\\d+\\s*$", .x))), ~ as.numeric(.)))
```

```{r}
# Step 5b: Handle factors and location labels
table_6 <- table_5 |> 
  mutate(
    result = as.factor(result),
    game_location = case_when(
      game_location == "@" ~ "away",
      game_location == "" ~ "home",
      TRUE ~ game_location
    ) |>  as.factor()
  )
```

```{r}
# Step 5c: Final column cleanup
table_7 <- table_6 |> 
  rename_with(~ str_replace(., "^_", ""), .cols = starts_with("_"))
```

*Discussion: Why convert categorical variables like `score_rslt` or
`game_location` to factors? What impact could that have on modeling or
plotting?*

------------------------------------------------------------------------

## Step 6: Wrap Process in a Function (Year Only)

```{r}
# Step 6: Year-only function
fn_year <- function(year) {
  # Step 1: Define team and year
team_name <- "was"

# Step 1a: Construct full URL
generic_url <- paste0("https://www.pro-football-reference.com/teams/", team_name, "/", year, ".htm#all_games")
  
 # Step 2: Read HTML page
webpage <- generic_url |> rvest::read_html()

# Step 2a: Extract all HTML tables
web_tables <- webpage |> rvest::html_table()

# Step 3: Pick the regular season game table (check structure visually)
int_web_table <- web_tables |> purrr::pluck(2)
 # Step 3a: Use first row as column names + clean them
firstrow_names <- int_web_table[1, ] |> unlist() |> as.character()

# Step 3b: Assign as column names
colnames(int_web_table) <- firstrow_names

# Step 3c: Remove the first row (it's now the header)
table_1 <- int_web_table[-1, ]

# Step 3d: Clean the column names with janitor
table_2 <- janitor::clean_names(table_1)

# Step 3e: Fix problem cases with no useful data within the header or first rows
table_3 <- table_2 |> 
  rename(
    result = x_3,
    game_location = x_4
)
# Step 4: Drop irrelevant columns and rows, keep only valid games
table_4 <- table_3 |> 
  select(!(x:x_2)) |> 
  filter(opp != "Bye Week")

# Step 4a: Convert numeric-looking strings to numeric
table_5 <- table_4 |>  
  mutate(across(where(~ all(grepl("^\\s*-?\\d*\\.?\\d+\\s*$", .x))), ~ as.numeric(.)))

# Step 4b: Handle factors and location labels
table_6 <- table_5 |> 
  mutate(
    result = as.factor(result),
    game_location = case_when(
      game_location == "@" ~ "away",
      game_location == "" ~ "home",
      TRUE ~ game_location
    ) |>  as.factor()
  )

# Step 4c: Final column cleanup
table_7 <- table_6 |> 
  rename_with(~ str_replace(., "^_", ""), .cols = starts_with("_"))

  return(table_7)
}
```

Test Year Only Function

```{r}
head(fn_year(2022))
```

## Step 7: Wrap Process in Full Function (Team + Year)

```{r}
# Step 7: Generalized function
fn_team_year <- function(team_name, year) {

# Step 2a: Construct full URL
generic_url <- paste0("https://www.pro-football-reference.com/teams/", team_name, "/", year, ".htm#all_games")
  
 # Step 3: Read HTML page
webpage <- generic_url |> rvest::read_html()

# Step 3a: Extract all HTML tables
web_tables <- webpage |> rvest::html_table()

# Step 3b: Pick the regular season game table (check structure visually)
int_web_table <- web_tables |> purrr::pluck(2)
 # Step 4a: Use first row as column names + clean them
firstrow_names <- int_web_table[1, ] |> unlist() |> as.character()

# Step 4b: Assign as column names
colnames(int_web_table) <- firstrow_names

# Step 4c: Remove the first row (it's now the header)
table_1 <- int_web_table[-1, ]

# Step 4d: Clean the column names with janitor
table_2 <- janitor::clean_names(table_1)

# Step 4e: Fix problem cases with no useful data within the header or first rows
table_3 <- table_2 |> 
  rename(
    result = x_3,
    game_location = x_4
)
# Step 5: Drop irrelevant columns and rows, keep only valid games
table_4 <- table_3 |> 
  select(!(x:x_2)) |> 
  filter(opp != "Bye Week")

# Step 5a: Convert numeric-looking strings to numeric
table_5 <- table_4 |>  
  mutate(across(where(~ all(grepl("^\\s*-?\\d*\\.?\\d+\\s*$", .x))), ~ as.numeric(.)))

# Step 5b: Handle factors and location labels
table_6 <- table_5 |> 
  mutate(
    result = as.factor(result),
    game_location = case_when(
      game_location == "@" ~ "away",
      game_location == "" ~ "home",
      TRUE ~ game_location
    ) |>  as.factor()
  )

# Step 5c: Final column cleanup
table_7 <- table_6 |> 
  rename_with(~ str_replace(., "^_", ""), .cols = starts_with("_"))

  return(table_7)
}
```

Test Function

```{r}
head(fn_team_year("sfo", 2024))
```

------------------------------------------------------------------------

## Step 8: Visualize Results

Use `ggplot2` to create simple and insightful visualizations.

```{r}
# Step 8: Line plot of points scored by Week
ggplot(fn_team_year("sfo", 2024), aes(x = week, y = tm)) +
  geom_line(color = "steelblue", linewidth = 1.2) +
  geom_point(size = 3) +
  labs(
    title = "Points Scored Over Time",
    x = "Week",
    y = "Points Scored"
  ) +
  theme_minimal()
```

```{r}
# Step 8a: Compare performance by game location
ggplot(fn_team_year("sfo", 2024), aes(x = game_location, y = tm, fill = game_location)) +
  geom_boxplot() +
  labs(
    title = "Points Scored: Home vs Away",
    x = "Location",
    y = "Points Scored"
  ) +
  theme_minimal()
```

*Bonus: How might you visualize win/loss trends over the season? Could
you include opponent information or passing yards?*

# Session 3: HTML Web Scraping: NFL Data Extraction

[Pro Football Reference Changed the format of the HTML table... yay!
(maybe httr2 tools can be used?)]

[Find places to ask questions, comments, discussions]

```{r}
#| eval: false

library(rvest)      # Web scraping
library(dplyr)      # Data manipulation
library(stringr)    # String modification
library(rlang)      # Support / evaluation
library(purrr)      # Enchanced iteration
library(ggplot2)    # Visualizations
```

## Step 1: Simplistic Call

Construct the URL by using the URL and filling in the team name and
year.

```{r}
#| eval: false
## Creation of URL
team_name <- "was"
year <- 2023
generic_url <- paste0("https://www.pro-football-reference.com/teams/",team_name,"/",year,".htm#all_games")
```

```{r}
## Special Note 1: Maybe Optional (Further Research is needed on its relevance)
# request_details <- "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
# response <- GET(url, user_agent(request_details),timeout(20))
```

## Step 3: Get HTML Tables

## Step 5: Consolidate Column Information

Now with our table selected, we need to find a way to include
information from the header as well as the first row.

Our first row contains our column names and the header contains the
column categories. In order to preserve this information, we will
combine the names of the header and first row. To do this, we will
create 2 vectors with the names, and then paste them together with a
separator of `_`. Next, using `str_trim()` and `str_to_lower()`, we
convert the names to snake case, or lower case with spaces represented
as underscore.

```{r}
#| eval: false
inital_col_names <- colnames(int_web_table)

first_row_names <- slice_head(int_web_table)
# CANNOT USE SLICE: CANNOT TRANSFORM A DATA FRAME WITH NA OR "" NAMES

new_colnames <- paste(inital_col_names,first_row_names,sep="_") %>% 
  str_trim() %>% 
  str_to_lower()
```

## Step 6: Fix Problem Columns

We have 2 issues with this process, one being that column 6 does not
contain any values in the first row or the header. As such, we will
assign a name to this column. Additionally, column 21 and column 15 have
a duplicate name, but one refers to passing yards and the other to
passing yards sacked. We will manually assign both of these to names to
avoid future issue, but this is not the preferred method of addressing
these issues.

{Question: What limitations does this pose?}

```{r}
#| eval: false
new_colnames[6] <- "game_location"
new_colnames[21] <- "passing_yds_sk"

```

## Step 7: Assign names to the data frame

Now we will take our new column names vector and assign it using
colnames() as the column names for the data frame.

```{r}
#| eval: false
colnames(int_web_table) <- new_colnames
```

## Step 8: Remove extraneous rows

Here we will use the dplyr::slice() function from dplyr to clip off the
first and last rows, as they do not contain useful information. We will
do this by selecting all of the content between those rows.

```{r}
#| eval: false
web_table1 <- int_web_table %>% 
 dplyr::slice(2:18)
```

## Step 9: Set Numeric Columns Correctly

Here we will use dplyr mutate() and across() to modify multiple columns
and use a Regex (regular expression) pattern to look for numeric-looking
strings and turn them into numeric types using as.numeric.

```{r}
#| eval: false
web_table2 <- web_table1 %>% 
 mutate(
    across(where(~ all(grepl("^\\s*-?\\d*\\.?\\d+\\s*$", .x))), ~ as.numeric(.)))
```

## Step 10: Address Non-Numeric Columns

Now, we will use mutate() again to change the score_rslt type to a
factor, and use case_when() to make more sense of the game_location
column. By identifying when '\@' appears, we can change it to away, and
the opposite '' as home. We also change this to a factor. Lastly, we
filter out the last column which is an aggregate column by filtering out
games with no date.

```{r}
#| eval: false
web_table3 <- web_table2  %>% 

  mutate(
    score_rslt = as.factor(score_rslt),
    game_location = case_when(
      game_location == "@" ~ "away",
      game_location == "" ~ "home",
      TRUE ~ game_location
    ) %>% 
      as.factor()
  )
```

{Question: Why change to a factor? What benefits could that have?}

## Step 11: Finish Cleaning Code and Data Frame

Lets clean up the column names again, by removing the underscore (\_)
from the front of the columns. Using dplyr::rename_with() and
stringr::str_replace(), as well as dplyr::starts_with(), we can remove
unnecessary underscores.

```{r}
#| eval: false
web_table4 <- web_table3 |> 
  dplyr::rename_with(~ str_replace(., "^_", ""), .cols = starts_with("_")
```
